{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de7a1eb8",
   "metadata": {},
   "source": [
    "# NLP Project\n",
    "> Natural language processing (NLP) is a subfield of computer science and artificial intelligence (AI) that uses machine learning to enable computers to understand and communicate with human language.\n",
    "\n",
    "### Stages of NLP:\n",
    "0. Importing important libraries and pre-setup\n",
    "1. Segmentation\n",
    "2. Tokenization\n",
    "3. Removal of Stop words\n",
    "4. Stemming and Lemmatization\n",
    "5. Part of Speech Tagging \n",
    "6. Named Entity Recognition (NER)\n",
    "\n",
    "We will be using `NLTK` library in Python for NLP tasks. \n",
    "> for Documentation, refer to: https://www.nltk.org/ \n",
    "\n",
    "We will be working with this text:\n",
    "Millions of people across the UK and beyond have celebrated the coronation of King Charles III - a symbolic ceremony combining a religious service and pageantry. The ceremony was held at Westminster Abbey, with the King becoming the 40th reigning monarch to be crowned there since 1066. Queen Camilla was crowned alongside him before a huge parade back to Buckingham Palace. Here's how the day of splendour and formality, which featured customs dating back more than 1,000 years, unfolded.\n",
    "\n",
    "> link: https://www.bbc.com/news/uk-65342840 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91750b7d",
   "metadata": {},
   "source": [
    "### Stage 0. Importing important Libraries and pre-setup\n",
    "\n",
    "for this project, we will be meeting two key libraries. These are:\n",
    "1. NLTK\n",
    "2. RE\n",
    "\n",
    "> 1. `NLTK` will be used for NLP task\n",
    "> 2. `RE` will be used for regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aac6251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d88cb",
   "metadata": {},
   "source": [
    "We will also setup `text` variable here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf4fd01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Millions of people across the UK and beyond have celebrated the coronation of King Charles III - a symbolic ceremony combining a religious service and pageantry. The ceremony was held at Westminster Abbey, with the King becoming the 40th reigning monarch to be crowned there since 1066. Queen Camilla was crowned alongside him before a huge parade back to Buckingham Palace. Here's how the day of splendour and formality, which featured customs dating back more than 1,000 years, unfolded.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Millions of people across the UK and beyond have celebrated the coronation of King Charles III - a symbolic ceremony combining a religious service and pageantry. The ceremony was held at Westminster Abbey, with the King becoming the 40th reigning monarch to be crowned there since 1066. Queen Camilla was crowned alongside him before a huge parade back to Buckingham Palace. Here's how the day of splendour and formality, which featured customs dating back more than 1,000 years, unfolded.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b84460",
   "metadata": {},
   "source": [
    "### Stage 1. Segmentation\n",
    "\n",
    "We will break our entirety of text into small sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43fca200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Kasarla\n",
      "[nltk_data]     Vishwaja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# download initially important packages\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d42db1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Millions of people across the UK and beyond have celebrated the coronation of King Charles III - a symbolic ceremony combining a religious service and pageantry.',\n",
       " 'The ceremony was held at Westminster Abbey, with the King becoming the 40th reigning monarch to be crowned there since 1066.',\n",
       " 'Queen Camilla was crowned alongside him before a huge parade back to Buckingham Palace.',\n",
       " \"Here's how the day of splendour and formality, which featured customs dating back more than 1,000 years, unfolded.\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22d95fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Millions of people across the UK and beyond have celebrated the coronation of King Charles III - a symbolic ceremony combining a religious service and pageantry.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1db92705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Millions of people across the UK and beyond have celebrated the coronation of King Charles III   a symbolic ceremony combining a religious service and pageantry '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove punctuations\n",
    "text = re.sub(r'[^a-zA-Z0-9]', ' ', sentences[0])\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8600c76",
   "metadata": {},
   "source": [
    "### Stage 2. Tokenization\n",
    "\n",
    "Tokenization is simply breaking our sentences into tokens (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94965058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Millions', 'of', 'people', 'across', 'the', 'UK', 'and', 'beyond', 'have', 'celebrated', 'the', 'coronation', 'of', 'King', 'Charles', 'III', 'a', 'symbolic', 'ceremony', 'combining', 'a', 'religious', 'service', 'and', 'pageantry'] "
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(text)\n",
    "print(words, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2586964",
   "metadata": {},
   "source": [
    "### Stage 3. Removal of Stop Words\n",
    "\n",
    "Stop words are the words that appear frequently in language, but carry almost none to very minimal meaning in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03e8d5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Kasarla\n",
      "[nltk_data]     Vishwaja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e15d865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "# list of stopwords in English language in our NLTK\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ac4b166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Millions', 'people', 'across', 'UK', 'beyond', 'celebrated', 'coronation', 'King', 'Charles', 'III', 'symbolic', 'ceremony', 'combining', 'religious', 'service', 'pageantry'] "
     ]
    }
   ],
   "source": [
    "# removing stopwords\n",
    "words = [w for w in words if w not in stopwords.words('english')]\n",
    "print(words, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c64b922",
   "metadata": {},
   "source": [
    "### Stage 4. Stemming and Lemmetization\n",
    "\n",
    "Stemming is finding the stem root of the words.\n",
    "Lemmetization is find the base word of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81812677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Kasarla\n",
      "[nltk_data]     Vishwaja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Kasarla\n",
      "[nltk_data]     Vishwaja\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b9d468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['million', 'peopl', 'across', 'uk', 'beyond', 'celebr', 'coron', 'king', 'charl', 'iii', 'symbol', 'ceremoni', 'combin', 'religi', 'servic', 'pageantri'] "
     ]
    }
   ],
   "source": [
    "# stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# reduce words to their stem form\n",
    "stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "print(stemmed, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bd82e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Millions', 'people', 'across', 'UK', 'beyond', 'celebrated', 'coronation', 'King', 'Charles', 'III', 'symbolic', 'ceremony', 'combining', 'religious', 'service', 'pageantry'] "
     ]
    }
   ],
   "source": [
    "# lemmetization\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# reduce words to their base form\n",
    "lemmetized = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "print(lemmetized, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b37d03",
   "metadata": {},
   "source": [
    "### Another Helping Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ce8a73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wait', 'wait', 'studi', 'studi', 'fairli', 'fair'] ['wait', 'waiting', 'study', 'studying', 'fairly', 'fairness'] "
     ]
    }
   ],
   "source": [
    "words2 = ['wait', 'waiting', 'studies', 'studying', 'fairly', 'fairness']\n",
    "\n",
    "# stemming\n",
    "stemmed2 = [PorterStemmer().stem(w) for w in words2]\n",
    "print(stemmed2, end=\" \")\n",
    "\n",
    "# lemmetization\n",
    "lemmetized2 = [WordNetLemmatizer().lemmatize(w) for w in words2]\n",
    "print(lemmetized2, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d093ec3",
   "metadata": {},
   "source": [
    "### Stage 5. Part of Speech Tagging\n",
    "\n",
    "Part of Speech_Tagging is the process of identifying the part of speech (such as noun, verb, adjective, adverb, etc.) of each word in a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e16dac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Kasarla Vishwaja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\Kasarla\n",
      "[nltk_data]     Vishwaja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf069b",
   "metadata": {},
   "source": [
    "### NLTK POS Tag List\n",
    "Below is the pos tag list of nltk as follows. There is a multiple tag list available in nltk, tag list showing in output as per word.\n",
    "\n",
    "- CC: It is the conjunction of coordinating\n",
    "- CD: It is a digit of cardinal\n",
    "- DT: It is the determiner\n",
    "- EX: Existential\n",
    "- FW: It is a foreign word\n",
    "- IN: Preposition and conjunction\n",
    "- JJ: Adjective\n",
    "- JJR and JJS: Adjective and superlative\n",
    "- LS: List marker\n",
    "- MD: Modal\n",
    "- NN: Singular noun\n",
    "- NNS, NNP, NNPS: Proper and plural noun\n",
    "- PDT: Predeterminer\n",
    "- WRB: Adverb of wh\n",
    "- WP$: Possessive wh\n",
    "- WP: Pronoun of wh\n",
    "- WDT: Determiner of wp\n",
    "- VBZ: Verb\n",
    "- VBP, VBN, VBG, VBD, VB: Forms of verbs\n",
    "- UH: Interjection\n",
    "- TO: To go\n",
    "- RP: Particle\n",
    "- RBS, RB, RBR: Adverb\n",
    "- PRP, PRP$: Pronoun personal and professional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cab135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Millions', 'NNS'),\n",
       " ('people', 'NNS'),\n",
       " ('across', 'IN'),\n",
       " ('UK', 'NNP'),\n",
       " ('beyond', 'IN'),\n",
       " ('celebrated', 'VBN'),\n",
       " ('coronation', 'NN'),\n",
       " ('King', 'NNP'),\n",
       " ('Charles', 'NNP'),\n",
       " ('III', 'NNP'),\n",
       " ('symbolic', 'JJ'),\n",
       " ('ceremony', 'NN'),\n",
       " ('combining', 'VBG'),\n",
       " ('religious', 'JJ'),\n",
       " ('service', 'NN'),\n",
       " ('pageantry', 'NN')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "# tag each word with their respective parts of speech\n",
    "pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4b4956",
   "metadata": {},
   "source": [
    "### Stage 6. Named Entity Recognition (NER)\n",
    "\n",
    "Named Entity Recognition (NER) is a means of identifying and categorizing named entities in unstructured text into predefined categories such as names of people, organizations, etc in structured text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9938b672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to C:\\Users\\Kasarla\n",
      "[nltk_data]     Vishwaja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\Kasarla Vishwaja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25c85363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Millions/NNS\n",
      "  of/IN\n",
      "  people/NNS\n",
      "  across/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION UK/NNP)\n",
      "  and/CC\n",
      "  beyond/IN\n",
      "  have/VBP\n",
      "  celebrated/VBN\n",
      "  the/DT\n",
      "  coronation/NN\n",
      "  of/IN\n",
      "  King/NNP\n",
      "  (PERSON Charles/NNP III/NNP)\n",
      "  -/:\n",
      "  a/DT\n",
      "  symbolic/JJ\n",
      "  ceremony/NN\n",
      "  combining/VBG\n",
      "  a/DT\n",
      "  religious/JJ\n",
      "  service/NN\n",
      "  and/CC\n",
      "  pageantry/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "ner_tree = ne_chunk(pos_tag(word_tokenize(sentences[0]))) # tokenize the sentences and then ne-chunk it\n",
    "print(ner_tree) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
